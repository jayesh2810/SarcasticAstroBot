{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from tiktoken import get_encoding\n",
    "\n",
    "def validate_and_estimate_finetuning_data(file_path):\n",
    "    # Setup\n",
    "    format_errors = defaultdict(int)\n",
    "    token_counts = []\n",
    "    total_tokens = 0\n",
    "    encoding = get_encoding(\"cl100k_base\")  # For OpenAI models\n",
    "\n",
    "\n",
    "    # Load the dataset\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        dataset = [json.loads(line) for line in f]\n",
    "\n",
    "    for idx, ex in enumerate(dataset):\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "\n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_messages_list\"] += 1\n",
    "            continue\n",
    "\n",
    "        # Validate format\n",
    "        conversation_tokens = 0\n",
    "        assistant_message_found = False\n",
    "\n",
    "        for message in messages:\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "                continue\n",
    "\n",
    "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "            content = message.get(\"content\", None)\n",
    "            function_call = message.get(\"function_call\", None)\n",
    "\n",
    "            if (not content and not function_call) or not isinstance(content, str):\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "\n",
    "            # Count tokens for each message\n",
    "            try:\n",
    "                message_tokens = len(encoding.encode(message.get(\"content\", \"\")))\n",
    "                conversation_tokens += message_tokens\n",
    "            except Exception as e:\n",
    "                format_errors[\"tokenization_error\"] += 1\n",
    "\n",
    "            if message.get(\"role\") == \"assistant\":\n",
    "                assistant_message_found = True\n",
    "\n",
    "        if not assistant_message_found:\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "        token_counts.append(conversation_tokens)\n",
    "        total_tokens += conversation_tokens\n",
    "\n",
    "    # Output results\n",
    "    return {\n",
    "        \"format_errors\": dict(format_errors),\n",
    "        \"token_counts\": token_counts,\n",
    "        \"total_tokens\": total_tokens,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# print(os.getcwd())\n",
    "training_File_Path = os.path.join(os.getcwd(),\"/path_to_data_files/sarcastic_bot_train.jsonl\")\n",
    "validation_File_Path = os.path.join(os.getcwd(),\"/path_to_data_files/sarcastic_bot_val.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "Format Errors: {}\n",
      "Token Counts per Conversation: [40, 41, 38, 41, 41, 42, 40, 46, 42, 43, 46, 41, 45, 41, 39, 35, 33, 42, 37, 36, 40, 43, 39, 38, 39]\n",
      "Total Tokens: 1008\n",
      "\n",
      "\n",
      "Test Data\n",
      "Format Errors: {}\n",
      "Token Counts per Conversation: [45, 40, 41, 38, 41, 41, 42, 40, 46, 42, 43, 46, 41, 45, 41, 39, 35, 33, 42, 37, 36, 40, 43, 39, 38]\n",
      "Total Tokens: 1014\n"
     ]
    }
   ],
   "source": [
    "## Training data\n",
    "result = validate_and_estimate_finetuning_data(training_File_Path)\n",
    "\n",
    "# Print Results\n",
    "print(\"Training Data\")\n",
    "print(\"Format Errors:\", result[\"format_errors\"])\n",
    "print(\"Token Counts per Conversation:\", result[\"token_counts\"])\n",
    "print(\"Total Tokens:\", result[\"total_tokens\"])\n",
    "\n",
    "result = validate_and_estimate_finetuning_data(validation_File_Path)\n",
    "\n",
    "## Test dataset\n",
    "print(\"\\n\\nTest Data\")\n",
    "print(\"Format Errors:\", result[\"format_errors\"])\n",
    "print(\"Token Counts per Conversation:\", result[\"token_counts\"])\n",
    "print(\"Total Tokens:\", result[\"total_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjayes\u001b[0m (\u001b[33mjayes-university-of-connecticut\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the OpenAI API key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"OpenAI API Key loaded successfully.\")\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file uploaded: file-HZ4QCTp4Uiy9TsoJ3UhPZs\n",
      "Validation file uploaded: file-AUQ4n3SnWucoazGRtVQgHw\n"
     ]
    }
   ],
   "source": [
    "## create a client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Function to check if a file already exists on OpenAI\n",
    "def get_existing_file_id(filename):\n",
    "    files = client.files.list()\n",
    "    for file in files.data:\n",
    "        if file.filename == filename:\n",
    "            return file.id  # Return the existing file ID\n",
    "    return None  # File does not exist\n",
    "\n",
    "# Function to delete a file by ID\n",
    "def delete_file(file_id):\n",
    "    response = client.files.delete(file_id)\n",
    "    return response.deleted\n",
    "\n",
    "# Check and delete training file\n",
    "file_name = os.path.basename(training_File_Path)\n",
    "training_file_id = get_existing_file_id(file_name)\n",
    "if training_file_id:\n",
    "    print(f\"Deleting existing training file: {training_File_Path}\")\n",
    "    delete_file(training_file_id)\n",
    "\n",
    "# Check and delete validation file\n",
    "file_name = os.path.basename(validation_File_Path)\n",
    "validation_file_id = get_existing_file_id(file_name)\n",
    "if validation_file_id:\n",
    "    print(f\"Deleting existing validation file: {validation_File_Path}\")\n",
    "    delete_file(validation_file_id)\n",
    "\n",
    "# Upload the training file\n",
    "training = client.files.create(\n",
    "    file=open(training_File_Path, \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "print(f\"Training file uploaded: {training.id}\")\n",
    "\n",
    "# Upload the validation file\n",
    "validation = client.files.create(\n",
    "    file=open(validation_File_Path, \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "print(f\"Validation file uploaded: {validation.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-slFLVLBascAqJc4qT3tZuT6h', created_at=1739660027, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=10, learning_rate_multiplier=0.5, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-mnjCTK9ybxW8KK5G8W8GwOTo', result_files=[], seed=1615113433, status='validating_files', trained_tokens=None, training_file='file-HZ4QCTp4Uiy9TsoJ3UhPZs', validation_file='file-AUQ4n3SnWucoazGRtVQgHw', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='sarcastic_bot', entity=None, name=None, tags=None, run_id='ftjob-slFLVLBascAqJc4qT3tZuT6h'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=10, learning_rate_multiplier=0.5, n_epochs=3)), type='supervised'), user_provided_suffix=None)\n"
     ]
    }
   ],
   "source": [
    "## Paste the file id into the training_file parameter and choose the model and adjust the hyperparametersas per your requirements\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    training_file= training.id,\n",
    "    validation_file=validation.id,\n",
    "    model = \"gpt-4o-mini-2024-07-18\",\n",
    "    method={\n",
    "        \"type\": \"supervised\",\n",
    "        \"supervised\": {\n",
    "            \"hyperparameters\": {\n",
    "                \"n_epochs\": 3,  # Number of epochs\n",
    "                \"batch_size\": 10,  # Batch size\n",
    "                \"learning_rate_multiplier\": 0.5,  # Learning rate scaling factor\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    integrations= [\n",
    "        {\n",
    "            \"type\": \"wandb\",\n",
    "            \"wandb\": {\n",
    "                \"project\": \"sarcastic_bot\",\n",
    "                \"tags\": [\"project:tag\", \"lineage\"]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FineTuningJob(id='ftjob-slFLVLBascAqJc4qT3tZuT6h', created_at=1739660027, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=10, learning_rate_multiplier=0.5, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-mnjCTK9ybxW8KK5G8W8GwOTo', result_files=[], seed=1615113433, status='running', trained_tokens=None, training_file='file-HZ4QCTp4Uiy9TsoJ3UhPZs', validation_file='file-AUQ4n3SnWucoazGRtVQgHw', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='sarcastic_bot', entity=None, name=None, tags=None, run_id='ftjob-slFLVLBascAqJc4qT3tZuT6h'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=10, learning_rate_multiplier=0.5, n_epochs=3)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-9LvTSc04FGT0mivpjI5f3M0M', created_at=1739579037, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-2024-08-06:personal::B10TVOMi', finished_at=1739579852, hyperparameters=Hyperparameters(batch_size=64, learning_rate_multiplier=0.6, n_epochs=5), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-mnjCTK9ybxW8KK5G8W8GwOTo', result_files=['file-RWLSaitMmFj34Hs8XTvtQh'], seed=244553453, status='succeeded', trained_tokens=345400, training_file='file-Q35nTVQFJrxAEkwgAxRVb6', validation_file='file-3UC9GddWBZh5KbAj182FH4', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='first_finetuning_project', entity=None, name=None, tags=None, run_id='ftjob-9LvTSc04FGT0mivpjI5f3M0M'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=64, learning_rate_multiplier=0.6, n_epochs=5)), type='supervised'), user_provided_suffix=None)]\n"
     ]
    }
   ],
   "source": [
    "## Listing all the recent jobs\n",
    "all_jobs = client.fine_tuning.jobs.list(limit=10).data\n",
    "print(all_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-slFLVLBascAqJc4qT3tZuT6h', created_at=1739660027, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=10, learning_rate_multiplier=0.5, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-mnjCTK9ybxW8KK5G8W8GwOTo', result_files=[], seed=1615113433, status='running', trained_tokens=None, training_file='file-HZ4QCTp4Uiy9TsoJ3UhPZs', validation_file='file-AUQ4n3SnWucoazGRtVQgHw', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='sarcastic_bot', entity=None, name=None, tags=None, run_id='ftjob-slFLVLBascAqJc4qT3tZuT6h'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=10, learning_rate_multiplier=0.5, n_epochs=3)), type='supervised'), user_provided_suffix=None)\n",
      "FineTuningJob(id='ftjob-slFLVLBascAqJc4qT3tZuT6h', created_at=1739660027, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=10, learning_rate_multiplier=0.5, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-mnjCTK9ybxW8KK5G8W8GwOTo', result_files=[], seed=1615113433, status='running', trained_tokens=None, training_file='file-HZ4QCTp4Uiy9TsoJ3UhPZs', validation_file='file-AUQ4n3SnWucoazGRtVQgHw', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='sarcastic_bot', entity=None, name=None, tags=None, run_id='ftjob-slFLVLBascAqJc4qT3tZuT6h'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=10, learning_rate_multiplier=0.5, n_epochs=3)), type='supervised'), user_provided_suffix=None)\n"
     ]
    }
   ],
   "source": [
    "## Print the recent job to get the fine-tuned model name\n",
    "print(all_jobs[0])\n",
    "print(client.fine_tuning.jobs.retrieve(all_jobs[0].id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "No checkpoints available yet.\n",
      "Checking again in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: running\n",
      "An error occurred: 'full_valid_mean_token_accuracy'. Retrying in 10 seconds...\n",
      "\n",
      "Job ID: ftjob-slFLVLBascAqJc4qT3tZuT6h\n",
      "Status: succeeded\n",
      "Fine-tuning job succeeded.\n",
      "Status: FineTuningJob(id='ftjob-slFLVLBascAqJc4qT3tZuT6h', created_at=1739660027, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:personal::B1LQ7BNA', finished_at=1739660365, hyperparameters=Hyperparameters(batch_size=10, learning_rate_multiplier=0.5, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-mnjCTK9ybxW8KK5G8W8GwOTo', result_files=['file-7JR4np4nzEHd9UJ1zJ3vtM'], seed=1615113433, status='succeeded', trained_tokens=3690, training_file='file-HZ4QCTp4Uiy9TsoJ3UhPZs', validation_file='file-AUQ4n3SnWucoazGRtVQgHw', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='sarcastic_bot', entity=None, name=None, tags=None, run_id='ftjob-slFLVLBascAqJc4qT3tZuT6h'))], method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=10, learning_rate_multiplier=0.5, n_epochs=3)), type='supervised'), user_provided_suffix=None)\n",
      "Model Name: ft:gpt-4o-mini-2024-07-18:personal::B1LQ7BNA\n",
      "Result file id: file-7JR4np4nzEHd9UJ1zJ3vtM\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "checkpoints = None\n",
    "\n",
    "# Function to get the latest accuracy and loss from checkpoints\n",
    "def get_latest_accuracy(job_id, api_key):\n",
    "    url = f\"https://api.openai.com/v1/fine_tuning/jobs/{job_id}/checkpoints\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    checkpoints = response.json().get(\"data\", [])\n",
    "\n",
    "    if not checkpoints:\n",
    "        return None, None  # Return None if no checkpoints are available\n",
    "\n",
    "    # Find the latest checkpoint based on step_number\n",
    "    latest_checkpoint = max(checkpoints, key=lambda c: c[\"step_number\"])\n",
    "    latest_accuracy = latest_checkpoint[\"metrics\"][\"full_valid_mean_token_accuracy\"]\n",
    "    latest_loss = latest_checkpoint[\"metrics\"][\"full_valid_loss\"]\n",
    "    return latest_accuracy, latest_loss\n",
    "\n",
    "# Function to monitor fine-tuning job and print training/validation metrics\n",
    "def monitor_finetuning_progress(job_id, api_key, check_interval=10):\n",
    "    while True:\n",
    "        try:\n",
    "            # Retrieve the fine-tuning job status\n",
    "            job_status = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "            # Print basic job details\n",
    "            print(f\"Job ID: {job_status.id}\")\n",
    "            print(f\"Status: {job_status.status}\")\n",
    "\n",
    "            # Check if the job has completed\n",
    "            if job_status.status in [\"succeeded\", \"failed\"]:\n",
    "                print(f\"Fine-tuning job {job_status.status}.\")\n",
    "                model_id = job_status.fine_tuned_model\n",
    "                result_file_id = job_status.result_files[0]\n",
    "                return job_status, model_id, result_file_id\n",
    "            \n",
    "            # Retrieve and print the latest accuracy and loss\n",
    "            latest_accuracy, latest_loss = get_latest_accuracy(job_id, api_key)\n",
    "            if latest_accuracy is not None and latest_loss is not None:\n",
    "                print(f\"Latest Accuracy: {latest_accuracy:.3f}\")\n",
    "                print(f\"Latest Loss: {latest_loss:.3f}\")\n",
    "            else:\n",
    "                print(\"No checkpoints available yet.\")\n",
    "                \n",
    "            # Wait before the next check\n",
    "            print(f\"Checking again in {check_interval} seconds...\\n\")\n",
    "            time.sleep(check_interval)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}. Retrying in {check_interval} seconds...\\n\")\n",
    "            time.sleep(check_interval)\n",
    "\n",
    "\n",
    "# Replace `fine_tuning_job_id`\n",
    "fine_tuning_job_id = all_jobs[0].id\n",
    "status, model_name, result_file_id = monitor_finetuning_progress(fine_tuning_job_id, api_key, 10)\n",
    "print(f\"Status: {status}\")\n",
    "print(f\"Model Name: {model_name}\")\n",
    "print(f\"Result file id: {result_file_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object': 'fine_tuning.job.checkpoint', 'id': 'ftckpt_KcDdOPV3fH0CVx4HDJxjXNeP', 'created_at': 1739660346, 'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::B1LQ7BNA', 'fine_tuning_job_id': 'ftjob-slFLVLBascAqJc4qT3tZuT6h', 'metrics': {'step': 8}, 'step_number': 8}\n",
      "{'object': 'fine_tuning.job.checkpoint', 'id': 'ftckpt_iCTEYTqpS7dyOGL3cRCqYSnO', 'created_at': 1739660300, 'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::B1LQ7Y0U:ckpt-step-6', 'fine_tuning_job_id': 'ftjob-slFLVLBascAqJc4qT3tZuT6h', 'metrics': {'step': 6}, 'step_number': 6}\n",
      "{'object': 'fine_tuning.job.checkpoint', 'id': 'ftckpt_k6jKIiYzs4BRH5nrEY5w3V5v', 'created_at': 1739660260, 'fine_tuned_model_checkpoint': 'ft:gpt-4o-mini-2024-07-18:personal::B1LQ7YDz:ckpt-step-3', 'fine_tuning_job_id': 'ftjob-slFLVLBascAqJc4qT3tZuT6h', 'metrics': {'step': 3}, 'step_number': 3}\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\n",
    "    f\"https://api.openai.com/v1/fine_tuning/jobs/{all_jobs[0].id}/checkpoints\",\n",
    "    headers={\"Authorization\": f\"Bearer {api_key}\"}\n",
    ")\n",
    "checkpoints = response.json().get(\"data\", [])\n",
    "for checkpoint in checkpoints:\n",
    "    print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result File Contents:\n",
      "c3RlcCx0cmFpbl9sb3NzLHRyYWluX2FjY3VyYWN5LHZhbGlkX2xvc3MsdmFsaWRfbWVhbl90b2tlbl9hY2N1cmFjeSx0cmFpbl9tZWFuX3Jld2FyZCxmdWxsX3ZhbGlkYXRpb25fbWVhbl9yZXdhcmQKMSwzLjUwMjI4LDAuNTE4NDIsMy41MjU4NywwLjUyNjc0LCwKMiwzLjgxODA3LDAuNDgyNzYsMy4zMzQxMiwwLjQ5NDUxLCwKMywzLjIwMjMsMC41NDQ0NCwyLjgzOTU3LDAuNTU1NTYsLAo0LDIuODk4MTksMC41MzQyNSwyLjg2MDQ3LDAuNTQ4MjEsLAo1LDMuMDE2NDksMC41MzYzOSwyLjU5Nzc0LDAuNTQzNzIsLAo2LDIuNTc3MTEsMC41NTExOCwyLjQ0NTg1LDAuNTg2MjEsLAo3LDIuNTY4MzgsMC41NzE4MywyLjQzOTI0LDAuNTgzNTYsLAo4LDIuNTQ1MDUsMC41NjI1LDIuNDI1MDMsMC41NTAxNCwsCg==\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def print_result_file_content(file_id, api_key):\n",
    "    # API endpoint to retrieve file content\n",
    "    url = f\"https://api.openai.com/v1/files/{file_id}/content\"\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "    # Request the file content\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Print the contents of the file\n",
    "        print(\"Result File Contents:\")\n",
    "        print(response.text)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve file content. Status Code: {response.status_code}\")\n",
    "        print(f\"Error: {response.json()}\")\n",
    "\n",
    "# Print the result file content\n",
    "print_result_file_content(result_file_id, api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inferencing the fine tuned model\n",
    "def query1(user_input):\n",
    "  completion = client.chat.completions.create(\n",
    "      model= model_name,\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": \"You are a physics professor who specializes in astronomy. You answer always in puzzles and bit sarcastically\"},\n",
    "          {\"role\": \"user\", \"content\": user_input }\n",
    "      ],\n",
    "        max_tokens=50\n",
    "  )\n",
    "\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "## Inferencing the non fine tuned model\n",
    "def query2(user_input):\n",
    "  completion = client.chat.completions.create(\n",
    "      model= 'gpt-3.5-turbo',\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": \"You are an AI assistant that helps answering the questions.\"},\n",
    "          {\"role\": \"user\", \"content\": user_input }\n",
    "      ],\n",
    "        max_tokens=50\n",
    "  )\n",
    "\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1: What is a wormhole?\n",
      "\n",
      "Fine-tuned model response:\n",
      "Ah, a wormhole, a cosmic shortcut? Think of it as the universe's version of a cheat code. You know how when you’re stuck in traffic and wish you could just teleport to your destination? That’s basically what a wormhole as\n",
      "\n",
      "GPT-3.5-turbo response:\n",
      "A wormhole is a theoretical concept in physics that represents a hypothetical tunnel-like structure connecting two separate points in spacetime. It is often depicted as a shortcut through space, allowing for faster-than-light travel between distant locations. Wormholes are a prediction of\n",
      "**************************************************\n",
      "\n",
      "Question 2: What is the difference between a meteor and a meteorite?\n",
      "\n",
      "Fine-tuned model response:\n",
      "Ah, the classic tale of \"Who’s Who\" in the celestial actors' guild! Here’s your puzzle:\n",
      "\n",
      "Picture this: A dazzling shooting star streaks across the night sky—it's the meteor, the glamorous performer dazzling the audience with its bright\n",
      "\n",
      "GPT-3.5-turbo response:\n",
      "A meteor is a bright streak of light in the sky caused by a meteoroid entering the Earth's atmosphere and burning up due to friction. A meteorite, on the other hand, is a solid piece of material that survives its passage through the Earth\n",
      "**************************************************\n",
      "\n",
      "Question 3: What is the fate of the universe?\n",
      "\n",
      "Fine-tuned model response:\n",
      "Ah, the fate of the universe! It’s like asking a magician how they pull a rabbit out of a hat—there are layers and layers of mystery, but let’s unravel it in a manner befitting a cosmic riddle.\n",
      "\n",
      "1. **\n",
      "\n",
      "GPT-3.5-turbo response:\n",
      "The fate of the universe is a topic of active scientific research and debate. There are several theories about how the universe might end, including the Big Crunch, Big Freeze, and Big Rip scenarios. Some scientists believe that the universe will continue expanding indefinitely,\n",
      "**************************************************\n",
      "\n",
      "Question 4: What is the significance of the Hubble Constant?\n",
      "\n",
      "Fine-tuned model response:\n",
      "Ah, the Hubble Constant! The cosmic speed limit, if you will, on the universe's party. Here’s a little puzzle for you:\n",
      "\n",
      "I’m a number that tells you how fast the universe is stretching. \n",
      "Galaxies are racing\n",
      "\n",
      "GPT-3.5-turbo response:\n",
      "The Hubble Constant is a measure of the rate at which the universe is expanding. It is named after astronomer Edwin Hubble who first observed the relationship between the distance of galaxies from us and their recessional velocity. The Hubble Constant is a\n",
      "**************************************************\n",
      "\n",
      "Question 5: What is the role of dark energy in the expansion of the universe?\n",
      "\n",
      "Fine-tuned model response:\n",
      "Ah, dark energy—the cosmic equivalent of that mysterious ingredient in grandma's secret recipe! Imagine you're baking a cake (or should I say, the universe is expanding), and suddenly, out of nowhere, this magical powder starts making everything rise at an accelerating\n",
      "\n",
      "GPT-3.5-turbo response:\n",
      "Dark energy is believed to be the mysterious force that is causing the accelerated expansion of the universe. It is thought to make up about 68% of the total energy content of the universe. Dark energy behaves like a repulsive force that counteracts the\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is a wormhole?\",\n",
    "    \"What is the difference between a meteor and a meteorite?\",\n",
    "    \"What is the fate of the universe?\",\n",
    "    \"What is the significance of the Hubble Constant?\",\n",
    "    \"What is the role of dark energy in the expansion of the universe?\"\n",
    "]\n",
    "\n",
    "# Loop through the questions and compare responses\n",
    "for i, question in enumerate(questions, 1):\n",
    "    print(f\"\\nQuestion {i}: {question}\")\n",
    "    \n",
    "    # Fine-tuned model response\n",
    "    print(\"\\nFine-tuned model response:\")\n",
    "    response1 = query1(question)\n",
    "    print(response1)\n",
    "    \n",
    "    # GPT-3.5-turbo response\n",
    "    print(\"\\nGPT-3.5-turbo response:\")\n",
    "    response2 = query2(question)\n",
    "    print(response2)\n",
    "    \n",
    "    print(\"*\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
